Numerical optimal control (GRAPE) can in principle discover pulse shapes that suppress all coherent gate error to machine precision. But when does that capability actually matter? We present a systematic comparison of Gaussian, DRAG, and GRAPE pulses for single-qubit gates on a three-level transmon model parameterized by IQM Garnet hardware ($T_1 = 37\,\mu$s, $T_2 = 9.6\,\mu$s,$\alpha/2\pi = -200$ MHz), with the explicit goal of identifying the regimes where numerical optimization provides genuine practical advantage over analytical methods.

Our central finding is that properly calibrated DRAG already operates near the decoherence floor. At 20 ns gate time, GRAPE eliminates all coherent error ($1 - F < 10^{-15}$), but DRAG achieves $1 - F = 4.9 \times 10^{-4}$ in coherent error alone,and $8.4 \times 10^{-4}$ under full decoherence --- only $1.2\times$ above GRAPE's decoherence-limited performance. More surprisingly,DRAG is \emph{more robust} than GRAPE to qubit frequency detuning (minimum fidelity 0.990 vs.\ 0.931 over $\pm 5$ MHz), the dominant calibration uncertainty in charge-noise-limited transmons. GRAPE retains superior amplitude robustness (minimum fidelity 0.994 vs.\ 0.990) and provides the only route to guaranteed zero coherent error, which matters at short gate times ($\lesssim 15$ ns) where perturbative corrections break down.

These results lead to concrete calibration guidance: (1) properly calibrated DRAG is sufficient for gate times $\gtrsim 20$ ns on hardware with $T_2/T \gtrsim 500$, (2) GRAPE is necessary at short gate times or when targeting error rates well below the decoherence floor, and (3) robust optimal control incorporating frequency uncertainty should be used when detuning is the dominant noise source. We decompose the full error budget (coherent, $T_1$, $T_2$, control noise) and provide the open-source QubitPulseOpt framework for reproducing all results.
